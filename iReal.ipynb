{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsoHD82aOW5hy8kFb1OxNU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pouyan6/ireal/blob/main/iReal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFSJ1Az4LYOU"
      },
      "source": [
        "!pip uninstall clip -y\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93bde396"
      },
      "source": [
        "!pip install ftfy regex tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libs"
      ],
      "metadata": {
        "id": "0BbHyRNmJ2TK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "zLv6ciMsJ6V_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "CDFKjx-UJ_RZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "_GffUVyTKG0t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load two images"
      ],
      "metadata": {
        "id": "45wuoyISLpKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img1 = preprocess(Image.open(\"house-1.jpg\")).unsqueeze(0).to(device)\n",
        "img2 = preprocess(Image.open(\"house-2.jpeg\")).unsqueeze(0).to(device)\n",
        "# img2 = preprocess(Image.open(\"house-3.png\")).unsqueeze(0).to(device)\n"
      ],
      "metadata": {
        "id": "k-u6o3d_Lupw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get embeddings"
      ],
      "metadata": {
        "id": "Jom3Yk9fOli2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    emb1 = model.encode_image(img1)\n",
        "    emb2 = model.encode_image(img2)"
      ],
      "metadata": {
        "id": "-xIoQevMOmVH"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalize & compute cosine similarity\n"
      ],
      "metadata": {
        "id": "ImnGO_EHOocz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = torch.cosine_similarity(emb1, emb2)\n",
        "print(\"Similarity score:\", similarity.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQn2wh09OqJo",
        "outputId": "48795e43-eae2-4f27-cf46-9c52041532b1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity score: 0.8495361804962158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example with DINOv2 (PyTorch)"
      ],
      "metadata": {
        "id": "rzEKaIvWRiv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO"
      ],
      "metadata": {
        "id": "Lb8Ahr8eRlwh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DINOv2\n",
        "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNlG2MlZRqYW",
        "outputId": "066a5d4f-18b7-444b-b068-d8a1c786f619"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vits14_pretrain.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84.2M/84.2M [00:00<00:00, 251MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DinoVisionTransformer(\n",
              "  (patch_embed): PatchEmbed(\n",
              "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
              "    (norm): Identity()\n",
              "  )\n",
              "  (blocks): ModuleList(\n",
              "    (0-11): 12 x NestedTensorBlock(\n",
              "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (attn): MemEffAttention(\n",
              "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls1): LayerScale()\n",
              "      (drop_path1): Identity()\n",
              "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (mlp): Mlp(\n",
              "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
              "        (act): GELU(approximate='none')\n",
              "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "        (drop): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (ls2): LayerScale()\n",
              "      (drop_path2): Identity()\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "  (head): Identity()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "transform = T.Compose([\n",
        "    T.Resize(256),\n",
        "    T.CenterCrop(224),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "Rv5iJG-CRvBm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(img_path):\n",
        "    img = Image.open(img_path).convert(\"RGB\")\n",
        "    img = transform(img).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        emb = model(img)\n",
        "    return emb"
      ],
      "metadata": {
        "id": "loc8IuJmRzrE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb1 = get_embedding(\"house-1.jpg\")\n",
        "emb2 = get_embedding(\"house-2.jpeg\")"
      ],
      "metadata": {
        "id": "ARNAb0yLR0hb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity = torch.cosine_similarity(emb1, emb2).item()\n",
        "print(\"Similarity score:\", similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvbkbE-oR83e",
        "outputId": "5cdddc06-4a90-45a3-af1f-b01fb02db365"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity score: 0.7294092178344727\n"
          ]
        }
      ]
    }
  ]
}